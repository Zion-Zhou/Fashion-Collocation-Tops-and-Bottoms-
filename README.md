# Fashion-Collocation-Tops-and-Bottoms-
The project was done in the class MSBA Managing and Mining Big Data(2019-2020) provided by the Programme Master of Science in Business Analytics from the University of Hong Kong and was done by ( in alphabetical order) Li Xiang, Song Cancan, Wang Yanyuan, Wu Yonglin, Yu Huizi, Zhou Zezhong, Zhu Menglin and Ryan Yao.

# 1.	Executive Summary
Women always feel that they have nothing to wear, even when faced with a full closet. In fact, what they lack is just the fashion idea. We are working for a professional APP developer, and our goal is to create an APP that recommends trendy clothing collocation based on their clothing item. Connected with multiple social media platforms, our APP understands the latest fashion style, and its recommendation system can help women address the problem of not knowing what to wear. 
Our project includes 4 phases: (1) Data collection, (2) Clothing detection, (3) Fashion collocation learning, and (4) Recommendation. 

# 2.	Introduction
Visual analysis of clothing is a topic that has received increasing attention in computer vision communities in recent years. Researchers have carried out multiple studies related to fashion since 2012, including fashion dataset (LIU et al. 2014)[1], clothes parsing (Yamaguchi et al. 2012)[2], fashion attributes detection (JIA et al. 2018)[3],  fashion style modeling (MA et al. 2017)[4], and fashion recommendation (CHEN and HE, 2018)[5].
We develop a fashion recommendation system based on algorithms on neural networks. The rest of the report is organized as follows. In Section 3, we introduce how we build a fashion database as a reference of good collocation. Section 4 would be divided into three parts, in which we introduce (1) YOLO,  an object detection algorithm, which we utilize to recognize the clothing items; (2) Triplet Neural Network, which we apply to compute the matching degree between tops and bottom; (3) implementation of our recommendation system. And finally, in Section 5, we will summarize the report and provide further development of our APP.

# 3.	Data
In order to build our outfits database and make it a reference of trendy collocation, we collected images of outfits in social media. We consider Weibo and Instagram popular platforms representing fashion trends for young people, and thus we select large accounts with millions of followers, and collect images from them. We totally collected 35996 images. As we only consider collocation between tops and bottoms, we include images with single top and bottom recognized, and eliminate the invalid ones. The final quantity of valid images is reduced to 2830. 

# 4.	Analysis
## 4.1 Object Detection
The first phase in our pipeline is to locate the specific position of the cloth in the image and to identify whether the cloth is a top or a bottom. In order to achieve the target of making our product user-friendly, we are confronted with a multiple-object detection issue, a state-of-art technique topic in computer vision.
Prevalent object detection frameworks are classified into two streamlines: (1) Two-stage Detector: Region-based Convolutional Neural Networks (R-CNNs) (Girshick et al., 2014) adopts a pre-trained CNN as the feature extractor on thousands of proposed region from the image and applies multiple SVCs for object classification. To tackle the downside of the speed attributed to multiple steps involved, Fast and Faster R-CNN are proposed by using regional proposal neural network (RPN) rather than the selective search (Girshick, 2015; Ren et al., 2015). (2) One-stage Detector: Single shot object detectors include SSD and YOLO (Liu et al., 2016). Specifically, SSD further speeds up the process by giving up using RPN and introduces the idea of multi-scale features and default boxes to offset the scarification in accuracy.
In order to achieve higher accuracy and faster speed, we adopt YOLOv3 (You Only Look Once) based on a complicated backbone (Darknet-53). Such a framework will split an image into an S×S grid cells, Object detection will be performed in the unit of grid cell. As for each grid cell, Yolov3 will first propose 3 anchor boxes and make adjustment to the bounding boxes in the training process. One key idea of Yolov3 is that predictions will be made across 3 scales, in which the small scale with a large receptive field is responsible for detecting large object while the large scale with a small receptive field is used for detecting small object, like colour. The three-layer feature maps are generated by up-sampling based on the previous layer. Owing to its shape is like pyramid, the structure is called Feature Pyramid Networks (FPN) that is designed for identifying multiple objects from one image, see the following figure.
In terms of the benchmark for the optimal anchor box, the idea of IoU (the percentage of the overlapping area between the anchor box and the ground truth in the union of the two) will be applied. Intuitively, such a metric somehow implies how confident we are about our detections. For each one of the bounding box, we will gain three sets of outputs, including the offsets that will tell us the location of the object detected, the objectness (confidence) score and the class probability, see the following figure.
As we are going to give recommendations later on based upon the user input, i.e., our main target is not object classification,  we only need to know whether the detection clothing is a bottom or a top, so we further modify the output classes of Yolov3 trained on deepfashion2 dataset in which there are 13 classes. Here, we only use 2 classes: tops and bottoms, and note that we remove those one-piece classes like dresses.
In summary, Yolov3 will interact with users, and after receiving the input from the user, we will get to know the location of the object and whether it’s a top or a bottom. Based upon that, we will chop the original image into two parts for later use (creating anchor, positive and negative in triplet neural network). If necessary, we can also tell you which subclass the object belongs to and how confident we are in terms of the detections.
## 4.2 Triplet Neural Network
Typical softmax based deep network wouldn’t help when the number of classes in the output layer is too high because of the sparseness of the network. In our model, more categories of fashion collocation will help improve the accuracy of the results, so we use triplet neural networks (TNNs) for training. The basic idea of TNNs is to learn a metric which automatically projects user-defined similar images to lie close while forcing dissimilar images to be far away in the embedding space. Three entities need to be defined-anchor, positive and negative. Our goal is to minimize the distance between anchor and the positive while maximizing the distance between anchor and the negative and finally classify multiple outfits. The network loss function can be described using a Euclidean distance function.
Anchor, positive and negative images are passed through their respective network and during backpropagation weight vectors are updated using shared architecture. During prediction time, any one network is used to compute the vector representation of input data. Then when we give a piece of clothes, the model can be used to generate a fit for us finding the one keeps the smallest distance to the one we give.
Next, we trained our model and tuned the hyper-parameters to promote the model accuracy on our test data. After trying numerous times we reach a lowest test loss as 0.1091, and a highest test accuracy as 79.31%. And we save the best model for further use. After training, the triplet neural network learns an embedding space where the similarity is ruled by the anchor-positive-negative triples in the training database. In our project, this similarity is the dressing rule. So, using this trained model, we can know the degree of matching between top and bottom through calculating their Euclidean distance in the embedding space.
## 4.3 Implementation
### 4.3.1 Process
Combining yolo neural network for detection and triplet neural network for recommendation, our project can finally be used in fashion outfit recommendation for customers. First we would ask the customer to input the url where the cloth image is stored. Then put the clothing image into the yolo network to detect whether it is a top or a bottom. After the detection, we would extract all the clothes in the category that differed with the input from our collection (eg. input is top, then we would match with all the bottoms in our fashion collection database) and calculate the Euclidean distance between all those clothes to the input through the triplet neural network. At the end, we would output the cloth that has the closest distance to the input as the best recommendation.
### 4.3.2 Result
The below graph shows some recommendation results that we get from the model. The left image is the query image and the next adjacent image is the recommended cloth retrieved from our fashion collection database. We can see that regardless of whether the input is top or bottom, we can get a corresponding match as output. Although it is difficult to accurately measure the collocation quality, roughly evaluate our results, the recommendation guidance is still reasonable, which shows the feasibility of our model and provides the possibility for further optimization.

# 5. Conclusion & Further Improvement
In conclusion, with the help of yolo neural network for detection and triplet neural network for recommendation, we have developed an intelligent APP that is able to learn the fashion collocation from the most recent trend and realizes the whole collocation recommendation process: recognizing the your own clothes style and recommending the most suitable collocation for you. 
In addition, we will make continuous efforts to explore business value for fAshIon. Firstly, we will seek cooperation opportunities with online shopping platforms to embed our APP and offer customized recommendation for their users. Secondly, we can add build-in e-commerce function to our APP so that customers can buy the recommended clothes conveniently. Besides, we can even act as an adviser, providing some inspiration to the designers since our APP can capture the latest fashion trend. To think further, our APP can also include some basic designing class. 
If we were given 6 months more, there are several places for improvement:
Firstly, we will upgrade our fashion dataset by increasing the number of source channels and the diversity of collocation types. Currently, most of our collections are from Weibo and Instagram. If time permits, we can collect data from other resources such as Xiaohongshu and Mogujie. We can even customize a database for a specific target market by crawling and selecting related graphs in that area. Besides, our current model can only recommend the tops or bottoms for the clients, while our detection model has the power to detect over 13 different types of collocations, such as shoes, bags etc. In the future, we would develop a more complicated network that can include more collocation types to recommend to our users. 
Secondly, we will continuously improve our model accuracy. On the one hand, we can train two different models based on different input type: off-body & on-body collocation and male & female collocation. In our current model, we do not divide our dataset into these two kinds, however, different collocation types may have different features, which may influence the model accuracy. On the other hand, we will consider and compare more sampling method then find the best model. As summarized in previous literature review, there exist other methods. Thus, we can try these methods, compare their accuracy results then finally find the most suitable model. All in all, its power and huge business potential make us have no reason to stop exploring it.

# 6. References
1.	Chen, Long and Yuhang He. “Dress Fashionably: Learn Fashion Collocation With Deep Mixed-Category Metric Learning.” AAAI (2018). 
2.	Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 580–587).
3.	Girshick, R.B. (2015). Fast R-CNN. 2015 IEEE International Conference on Computer Vision (ICCV), 1440-1448.
4.	Jia, Menglin, Yichen Zhou, Mengyun Shi, and Bharath Hariharan. (2018)"A Deep-Learning-Based Fashion Attributes Detection Model.". Web.
5.	Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C., & Berg, A.C. (2016). SSD: Single Shot MultiBox Detector. ECCV.
6.	Redmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ArXiv, abs/1804.02767.
7.	Redmon, J., Divvala, S.K., Girshick, R.B., & Farhadi, A. (2015). You Only Look Once: Unified, Real-Time Object Detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 779-788.
8.	S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015. 5, 6, 7 
9.	Yamaguchi, Kota & Kiapour, M.H. & Ortiz, L.E. & Berg, T.L. (2012). “Parsing clothing in fashion photographs.” Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 3570-3577. 10.1109/CVPR.2012.6248101.
10.	Yihui Ma, Jia Jia, Suping Zhou, Jingtian Fu, Yejun Liu,  Zijian Tong (2017), “Towards Better Understanding the Clothing Fashion Styles: A Multimodal Deep Learning Approach” AAAI
11.	Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang (2016). "DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations." 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016: 1096-104. Web.
12.	https://github.com/simaiden/Clothes-Detection.
13.	https://github.com/andreasveit/triplet-network-pytorch.
14.	https://github.com/switchablenorms/DeepFashion2.
